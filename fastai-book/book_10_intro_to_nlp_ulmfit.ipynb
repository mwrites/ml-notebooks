{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastai in /usr/local/lib/python3.10/dist-packages (2.7.14)\n",
      "Requirement already satisfied: nbdev in /usr/local/lib/python3.10/dist-packages (2.3.13)\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from fastai) (23.3.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastai) (23.2)\n",
      "Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from fastai) (0.0.7)\n",
      "Requirement already satisfied: fastcore<1.6,>=1.5.29 in /usr/local/lib/python3.10/dist-packages (from fastai) (1.5.29)\n",
      "Requirement already satisfied: torchvision>=0.11 in /usr/local/lib/python3.10/dist-packages (from fastai) (0.16.0+cu118)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from fastai) (3.8.3)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fastai) (2.2.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fastai) (2.31.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from fastai) (6.0.1)\n",
      "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.10/dist-packages (from fastai) (1.0.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from fastai) (9.3.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from fastai) (1.4.1.post1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from fastai) (1.12.0)\n",
      "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.10/dist-packages (from fastai) (3.7.4)\n",
      "Requirement already satisfied: torch<2.3,>=1.10 in /usr/local/lib/python3.10/dist-packages (from fastai) (2.1.0+cu118)\n",
      "Requirement already satisfied: ipywidgets<=8.0.4 in /usr/local/lib/python3.10/dist-packages (from nbdev) (8.0.4)\n",
      "Requirement already satisfied: execnb>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from nbdev) (0.1.5)\n",
      "Requirement already satisfied: astunparse in /usr/local/lib/python3.10/dist-packages (from nbdev) (1.6.3)\n",
      "Requirement already satisfied: ghapi>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from nbdev) (1.0.4)\n",
      "Requirement already satisfied: watchdog in /usr/local/lib/python3.10/dist-packages (from nbdev) (4.0.0)\n",
      "Requirement already satisfied: asttokens in /usr/local/lib/python3.10/dist-packages (from nbdev) (2.4.1)\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from execnb>=0.1.4->nbdev) (8.17.2)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<=8.0.4->nbdev) (6.26.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<=8.0.4->nbdev) (5.13.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<=8.0.4->nbdev) (4.0.9)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<=8.0.4->nbdev) (3.0.9)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (4.66.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (2.6.4)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (68.2.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (1.24.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fastai) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fastai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fastai) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fastai) (2024.2.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=1.10->fastai) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=1.10->fastai) (4.10.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=1.10->fastai) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=1.10->fastai) (3.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=1.10->fastai) (2023.4.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=1.10->fastai) (2.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from asttokens->nbdev) (1.16.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse->nbdev) (0.41.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib->fastai) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fastai) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->fastai) (2024.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fastai) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fastai) (3.4.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets<=8.0.4->nbdev) (0.2.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets<=8.0.4->nbdev) (1.8.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets<=8.0.4->nbdev) (7.4.9)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets<=8.0.4->nbdev) (5.5.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets<=8.0.4->nbdev) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets<=8.0.4->nbdev) (1.5.8)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets<=8.0.4->nbdev) (5.9.6)\n",
      "Requirement already satisfied: pyzmq>=20 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets<=8.0.4->nbdev) (24.0.1)\n",
      "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets<=8.0.4->nbdev) (6.3.3)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->execnb>=0.1.4->nbdev) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->execnb>=0.1.4->nbdev) (0.19.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /usr/local/lib/python3.10/dist-packages (from ipython->execnb>=0.1.4->nbdev) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from ipython->execnb>=0.1.4->nbdev) (2.16.1)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.10/dist-packages (from ipython->execnb>=0.1.4->nbdev) (0.6.3)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from ipython->execnb>=0.1.4->nbdev) (1.1.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->execnb>=0.1.4->nbdev) (4.8.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai) (2.16.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4->fastai) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4->fastai) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<4->fastai) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<4->fastai) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4->fastai) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.3,>=1.10->fastai) (1.3.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->execnb>=0.1.4->nbdev) (0.8.3)\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets<=8.0.4->nbdev) (0.4)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets<=8.0.4->nbdev) (3.11.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->execnb>=0.1.4->nbdev) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython->execnb>=0.1.4->nbdev) (0.2.9)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython->execnb>=0.1.4->nbdev) (2.0.1)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython->execnb>=0.1.4->nbdev) (0.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install fastai nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#100000) [Path('/root/.fastai/data/imdb/test/neg/6201_1.txt'),Path('/root/.fastai/data/imdb/test/neg/11881_2.txt'),Path('/root/.fastai/data/imdb/test/neg/11690_1.txt'),Path('/root/.fastai/data/imdb/test/neg/7300_1.txt'),Path('/root/.fastai/data/imdb/test/neg/2931_1.txt'),Path('/root/.fastai/data/imdb/test/neg/11165_4.txt'),Path('/root/.fastai/data/imdb/test/neg/1026_2.txt'),Path('/root/.fastai/data/imdb/test/neg/11499_3.txt'),Path('/root/.fastai/data/imdb/test/neg/3201_1.txt'),Path('/root/.fastai/data/imdb/test/neg/429_3.txt')...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastai.text.all import *\n",
    "path = untar_data(URLs.IMDB)\n",
    "files = get_text_files(path, folders = ['train', 'test', 'unsup'])\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I didn't know what to expect when I started watching this movie, by the end of it I was pulling my hairs out. This was one of the most pathetic movies of this year...in fact, in the last ten years. David Dhawan should just give up his career as a director. I am yet to come across one original script that David Dhawan has worked on. This one was a complete bit y bit rip off Hitch. I have nothing against remakes as such, but this one is just so lousy that it makes you even hate the original one (which was pretty decent). I fail to understand what actors like Salman and Govinda saw in this script. I read somewhere, that this was supposed to be Govinda's comeback vehicle. If thats true, then only God can save his career. Salman just overacted to the hilt. Govinda who I think is an actor of very high caliber was completely wasted. Katrina Kaif and LAra Dutta had nothing to do apart form wearing designer clothes and smiling for no rhyme or reason. Please stay away form this one!\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txts = L(o.open().read() for o in files[:2000])\n",
    "txts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy = WordTokenizer()\n",
    "tkz = Tokenizer(spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#231) ['xxbos','i','did',\"n't\",'know','what','to','expect','when','i'...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks200 = txts[:200].map(tkz)\n",
    "toks200[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(#1880) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the','.',',','a','of','and','to','is','it','i','this'...]\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = Numericalize()\n",
    "num.setup(toks200)\n",
    "coll_repr(num.vocab,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 72]), torch.Size([64, 72]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums200 = toks200.map(num)\n",
    "dl = LMDataLoader(nums200)\n",
    "x,y = first(dl)\n",
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"xxbos i did n't know what to expect when i started watching this movie , by the end of it\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify our X variable (should end with second last)\n",
    "' '.join(num.vocab[o] for o in x[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i did n't know what to expect when i started watching this movie , by the end of it i\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify our Y variable (should start from second token, and end with last)\n",
    "' '.join(num.vocab[o] for o in y[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 72]), torch.Size([64, 72]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_numericalize(txts):\n",
    "    # choose a tokenizer\n",
    "    spacy = WordTokenizer()\n",
    "    tkz = Tokenizer(spacy)\n",
    "    # tokenize the first 200 reviews\n",
    "    tokens = txts.map(tkz)\n",
    "    # numericalize\n",
    "    numericalized = tokens.map(num)\n",
    "    return tokens.map(num)\n",
    "\n",
    "\n",
    "# process the first 200 reviews\n",
    "numericalized = tokenize_numericalize(txts[:200])\n",
    "\n",
    "# feed that to the Language Model Data Loader\n",
    "dl = LMDataLoader(numericalized)\n",
    "x,y = first(dl)\n",
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Text Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numericalization can be done automatically when using `TextBlock`, it's also optimal as it:\n",
    "- save the tokenized documents in a tmp folder, so it doesn't have to tokenize them more than once\n",
    "- It runs multiple tokenization process in parallel, to take advcantage of your CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/root/.fastai/data/imdb')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_block = TextBlock.from_folder(path, is_lm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting-up type transforms pipelines\n",
      "Collecting items from /root/.fastai/data/imdb\n",
      "Found 100000 items\n",
      "2 datasets of sizes 90000,10000\n",
      "Setting up Pipeline: Tokenizer -> Numericalize\n",
      "\n",
      "Building one sample\n",
      "  Pipeline: Tokenizer -> Numericalize\n",
      "    starting from\n",
      "      /root/.fastai/data/imdb/train/pos/6659_10.txt\n",
      "    applying Tokenizer gives\n",
      "      ['xxbos', 'xxmaj', 'it', 'takes', 'guts', 'to', 'make', 'a', 'movie', 'on', 'xxmaj', 'gandhi', 'in', 'xxmaj', 'india', ',', 'in', 'which', 'he', 'is', 'not', 'shown', 'as', 'a', 'man', 'who', 'could', 'do', 'no', 'wrong.this', 'movie', 'shows', 'how', 'a', 'xxmaj', 'mahatma', 'failed', 'to', 'be', 'a', 'decent', 'father(at', 'least', 'in', 'the', 'eyes', 'of', 'his', 'son', ')', '.', '\\n\\n', 'xxmaj', 'the', 'performances', 'are', 'terrific', ',', 'the', 'cinematography', 'fantastic', ',', 'the', 'direction', 'fabulous', ',', 'but', 'the', 'film', 'drags.if', 'the', 'intention', 'was', 'to', 'make', 'this', 'movie', 'without', 'any', 'box', '-', 'office', 'expectations', ',', 'which', 'i', 'assume', 'is', 'the', 'case', 'here', ',', 'then', 'its', 'a', 'brilliant', 'attempt', ',', 'but', 'if', 'the', 'makers', 'were', 'expecting', 'this', 'to', 'be', 'a', 'commercial', 'success', ',', 'then', 'the', 'film', \"'s\", 'fate', 'was', 'doomed', 'the', 'day', 'they', 'chose', 'this', 'subject', '..', '\\n\\n', '20', 'yrs', 'from', 'now', ',', 'this', 'movie', 'will', 'be', 'remembered', 'for', 'the', 'brilliant', 'portrayal', 'of', 'xxmaj', 'harilal', 'by', 'xxmaj', 'akshaye', 'khanna.he', 'deserves', 'an', 'xxmaj', 'oscar', 'nomination', 'for', 'this', 'one', '..', 'and', 'honestly', ',', 'his', 'is', 'not', 'the', 'only', 'performance', 'worth', 'applauding', ',', 'xxmaj', 'shefali', 'xxmaj', 'chhaya', 'is', 'terrific', 'too', '..', '\\n\\n', 'xxmaj', 'watch', 'the', 'scene', 'where', 'xxmaj', 'harilal', 'hears', 'about', 'his', 'father', \"'s\", 'death.no', 'dialogues', ',', 'no', 'screaming', ',', 'but', 'a', 'speechless', 'shot', 'by', 'khanna.its', 'one', 'of', 'the', 'finest', 'scenes', 'ever', 'shot', 'in', 'the', 'history', 'of', 'xxmaj', 'cinema', '\\n\\n', 'gandhi', ',', 'my', 'xxmaj', 'father', 'is', 'not', 'at', 'all', 'exciting', 'cinema', 'but', 'yes', ',', 'its', 'excellent', 'cinema', 'and', 'a', 'must', 'watch.brilliant', 'xxmaj', 'attempt', '..']\n",
      "    applying Numericalize gives\n",
      "      TensorText of size 239\n",
      "\n",
      "Final sample: (TensorText([    2,     8,    17,   327,  3367,    15,   114,    13,    30,\n",
      "               35,     8,  7985,    18,     8,  2425,    11,    18,    82,\n",
      "               40,    16,    38,   640,    27,    13,   146,    50,    96,\n",
      "               59,    75,     0,    30,   296,   110,    13,     8, 23708,\n",
      "             1226,    15,    43,    13,   547,     0,   243,    18,     9,\n",
      "              548,    14,    41,   481,    34,    10,    26,     8,     9,\n",
      "              378,    39,  1361,    11,     9,   665,   840,    11,     9,\n",
      "              471,  2911,    11,    31,     9,    32,     0,     9,  3636,\n",
      "               25,    15,   114,    20,    30,   229,   121,   887,    24,\n",
      "             1050,  1435,    11,    82,    19,  2261,    16,     9,   438,\n",
      "              150,    11,   116,   115,    13,   549,   596,    11,    31,\n",
      "               63,     9,  1204,    88,  1043,    20,    15,    43,    13,\n",
      "             2106,  1020,    11,   116,     9,    32,    23,  2036,    25,\n",
      "             3811,     9,   273,    47,  2710,    20,   862,   334,    26,\n",
      "              852, 12707,    51,   168,    11,    20,    30,   106,    43,\n",
      "             2077,    29,     9,   549,  1124,    14,     8, 19328,    48,\n",
      "                8, 14747,     0,  1032,    49,     8,   866,  4400,    29,\n",
      "               20,    44,   334,    12,  1224,    11,    41,    16,    38,\n",
      "                9,    83,   261,   295, 24060,    11,     8, 26229,     8,\n",
      "                0,    16,  1361,   118,   334,    26,     8,   127,     9,\n",
      "              151,   136,     8, 19328,  5374,    60,    41,   360,    23,\n",
      "                0,  3700,    11,    75,  2179,    11,    31,    13, 10214,\n",
      "              347,    48,     0,    44,    14,     9,  2044,   158,   145,\n",
      "              347,    18,     9,   499,    14,     8,   464,    26,  7985,\n",
      "               11,    79,     8,   360,    16,    38,    46,    45,  1119,\n",
      "              464,    31,   445,    11,   115,   346,   464,    12,    13,\n",
      "              225,     0,     8,   596,   334]),)\n",
      "\n",
      "\n",
      "Collecting items from /root/.fastai/data/imdb\n",
      "Found 100000 items\n",
      "2 datasets of sizes 90000,10000\n",
      "Setting up Pipeline: Tokenizer -> Numericalize\n",
      "Setting up after_item: Pipeline: ToTensor\n",
      "Setting up before_batch: Pipeline: \n",
      "Setting up after_batch: Pipeline: \n",
      "\n",
      "Building one batch\n",
      "Applying item_tfms to the first sample:\n",
      "  Pipeline: ToTensor\n",
      "    starting from\n",
      "      (TensorText of size 239)\n",
      "    applying ToTensor gives\n",
      "      (TensorText of size 239)\n",
      "\n",
      "Adding the next 127 samples\n",
      "\n",
      "No before_batch transform to apply\n",
      "\n",
      "Collating items in a batch\n",
      "Error! It's not possible to collate your items in a batch\n",
      "Could not collate the 0-th members of your tuples because got the following shapes\n",
      "torch.Size([239]),torch.Size([254]),torch.Size([178]),torch.Size([268]),torch.Size([286]),torch.Size([381]),torch.Size([340]),torch.Size([257]),torch.Size([152]),torch.Size([266]),torch.Size([806]),torch.Size([199]),torch.Size([275]),torch.Size([324]),torch.Size([122]),torch.Size([167]),torch.Size([97]),torch.Size([179]),torch.Size([362]),torch.Size([497]),torch.Size([717]),torch.Size([178]),torch.Size([60]),torch.Size([155]),torch.Size([511]),torch.Size([996]),torch.Size([336]),torch.Size([71]),torch.Size([88]),torch.Size([378]),torch.Size([319]),torch.Size([236]),torch.Size([213]),torch.Size([310]),torch.Size([807]),torch.Size([481]),torch.Size([164]),torch.Size([148]),torch.Size([479]),torch.Size([219]),torch.Size([420]),torch.Size([107]),torch.Size([207]),torch.Size([305]),torch.Size([276]),torch.Size([332]),torch.Size([165]),torch.Size([188]),torch.Size([210]),torch.Size([200]),torch.Size([34]),torch.Size([367]),torch.Size([180]),torch.Size([201]),torch.Size([142]),torch.Size([163]),torch.Size([399]),torch.Size([159]),torch.Size([480]),torch.Size([903]),torch.Size([308]),torch.Size([165]),torch.Size([177]),torch.Size([782]),torch.Size([121]),torch.Size([163]),torch.Size([240]),torch.Size([194]),torch.Size([430]),torch.Size([463]),torch.Size([288]),torch.Size([172]),torch.Size([227]),torch.Size([266]),torch.Size([164]),torch.Size([150]),torch.Size([511]),torch.Size([335]),torch.Size([102]),torch.Size([1306]),torch.Size([298]),torch.Size([246]),torch.Size([208]),torch.Size([180]),torch.Size([237]),torch.Size([63]),torch.Size([409]),torch.Size([203]),torch.Size([304]),torch.Size([618]),torch.Size([240]),torch.Size([200]),torch.Size([151]),torch.Size([79]),torch.Size([176]),torch.Size([51]),torch.Size([228]),torch.Size([295]),torch.Size([597]),torch.Size([257]),torch.Size([271]),torch.Size([191]),torch.Size([75]),torch.Size([992]),torch.Size([236]),torch.Size([364]),torch.Size([1238]),torch.Size([176]),torch.Size([783]),torch.Size([159]),torch.Size([272]),torch.Size([158]),torch.Size([248]),torch.Size([182]),torch.Size([614]),torch.Size([338]),torch.Size([182]),torch.Size([176]),torch.Size([583]),torch.Size([334]),torch.Size([136]),torch.Size([141]),torch.Size([254]),torch.Size([251]),torch.Size([240]),torch.Size([156]),torch.Size([162]),torch.Size([204])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error when trying to collate the data into batches with fa_collate, at least two tensors in the batch are not the same size.\n\nMismatch found on axis 0 of the batch and is of type `TensorText`:\n\tItem at index 0 has shape: torch.Size([239])\n\tItem at index 1 has shape: torch.Size([254])\n\nPlease include a transform in `after_item` that ensures all data of type TensorText is the same size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m get_imdb \u001b[38;5;241m=\u001b[39m partial(get_text_files, folders\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munsup\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m block \u001b[38;5;241m=\u001b[39m DataBlock(\n\u001b[1;32m      4\u001b[0m     blocks\u001b[38;5;241m=\u001b[39mTextBlock\u001b[38;5;241m.\u001b[39mfrom_folder(path, is_lm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      5\u001b[0m     get_items\u001b[38;5;241m=\u001b[39mget_imdb, splitter\u001b[38;5;241m=\u001b[39mRandomSplitter(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m \u001b[43mblock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/data/block.py:237\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(self, source, bs, show_batch, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m     why \u001b[38;5;241m=\u001b[39m _find_fail_collate(s)\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure all parts of your samples are tensors of the same size\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m why \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m why)\n\u001b[0;32m--> 237\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m([f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m dls\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mafter_batch\u001b[38;5;241m.\u001b[39mfs \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnoop\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mApplying batch_tfms to the batch built\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/data/block.py:231\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(self, source, bs, show_batch, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCollating items in a batch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 231\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[43mdls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m     b \u001b[38;5;241m=\u001b[39m retain_types(b, s[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m is_listy(s) \u001b[38;5;28;01melse\u001b[39;00m s)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/data/load.py:181\u001b[0m, in \u001b[0;36mDataLoader.create_batch\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m (fa_collate,fa_convert)[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprebatched](b)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprebatched: \u001b[43mcollate_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/data/load.py:179\u001b[0m, in \u001b[0;36mDataLoader.create_batch\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, b): \n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43mfa_collate\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfa_convert\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprebatched\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \n\u001b[1;32m    181\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprebatched: collate_error(e,b)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/data/load.py:52\u001b[0m, in \u001b[0;36mfa_collate\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA replacement for PyTorch `default_collate` which maintains types and handles `Sequence`s\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m b \u001b[38;5;241m=\u001b[39m t[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (default_collate(t) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(b, _collate_types)\n\u001b[0;32m---> 52\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(t[\u001b[38;5;241m0\u001b[39m])([fa_collate(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mt)]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(b, Sequence)\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m default_collate(t))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/data/load.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA replacement for PyTorch `default_collate` which maintains types and handles `Sequence`s\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m b \u001b[38;5;241m=\u001b[39m t[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (default_collate(t) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(b, _collate_types)\n\u001b[0;32m---> 52\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(t[\u001b[38;5;241m0\u001b[39m])([\u001b[43mfa_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mt)]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(b, Sequence)\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m default_collate(t))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/data/load.py:51\u001b[0m, in \u001b[0;36mfa_collate\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA replacement for PyTorch `default_collate` which maintains types and handles `Sequence`s\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m b \u001b[38;5;241m=\u001b[39m t[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(b, _collate_types)\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(t[\u001b[38;5;241m0\u001b[39m])([fa_collate(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mt)]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(b, Sequence)\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m default_collate(t))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py:123\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n\u001b[0;32m--> 123\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcollate_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMapping):\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/torch_core.py:382\u001b[0m, in \u001b[0;36mTensorBase.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mdebug \u001b[38;5;129;01mand\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__str__\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__repr__\u001b[39m\u001b[38;5;124m'\u001b[39m): \u001b[38;5;28mprint\u001b[39m(func, types, args, kwargs)\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _torch_handled(args, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_opt, func): types \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mTensor,)\n\u001b[0;32m--> 382\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mifnone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m dict_objs \u001b[38;5;241m=\u001b[39m _find_args(args) \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;28;01melse\u001b[39;00m _find_args(\u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mtype\u001b[39m(res),TensorBase) \u001b[38;5;129;01mand\u001b[39;00m dict_objs: res\u001b[38;5;241m.\u001b[39mset_meta(dict_objs[\u001b[38;5;241m0\u001b[39m],as_copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:1386\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m   1385\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m-> 1386\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1387\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m get_default_nowrap_functions():\n\u001b[1;32m   1388\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error when trying to collate the data into batches with fa_collate, at least two tensors in the batch are not the same size.\n\nMismatch found on axis 0 of the batch and is of type `TensorText`:\n\tItem at index 0 has shape: torch.Size([239])\n\tItem at index 1 has shape: torch.Size([254])\n\nPlease include a transform in `after_item` that ensures all data of type TensorText is the same size"
     ]
    }
   ],
   "source": [
    "get_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])\n",
    "\n",
    "block = DataBlock(\n",
    "    blocks=TextBlock.from_folder(path, is_lm=True),\n",
    "    get_items=get_imdb, splitter=RandomSplitter(0.1)\n",
    ")\n",
    "\n",
    "block.summary(path, bs=128, seq_len=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on't forget about DataBlock's handy summary method, which is very useful for debugging data issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj its not the cast . a finer group of actors , you could not find . xxmaj its not the setting . xxmaj the director is in love with xxmaj new xxmaj york xxmaj city , and by the end of the film , so are we all ! xxmaj woody xxmaj allen could not improve upon what xxmaj bogdonovich has done here . xxmaj if you are going to fall in love , or find love ,</td>\n",
       "      <td>xxmaj its not the cast . a finer group of actors , you could not find . xxmaj its not the setting . xxmaj the director is in love with xxmaj new xxmaj york xxmaj city , and by the end of the film , so are we all ! xxmaj woody xxmaj allen could not improve upon what xxmaj bogdonovich has done here . xxmaj if you are going to fall in love , or find love , xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>looking at this film then either borrow it for free xxrep 3 ! or rent it on your unlimited rental plan at your nearby video store . xxmaj do n't pay money to watch this xxrep 3 ! i wonder if they will take a return if i say the disc do n't work ? \\n\\n xxmaj basically the story line is 4 college students get recruited to work on an independent film . xxmaj during the filming process ,</td>\n",
       "      <td>at this film then either borrow it for free xxrep 3 ! or rent it on your unlimited rental plan at your nearby video store . xxmaj do n't pay money to watch this xxrep 3 ! i wonder if they will take a return if i say the disc do n't work ? \\n\\n xxmaj basically the story line is 4 college students get recruited to work on an independent film . xxmaj during the filming process , things</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>but it 's not half bad . \\n\\n xxmaj still waiting for ' surf 3 ' . xxbos xxmaj this has got to be the funniest movie i have seen in forever . xxmaj xxunk xxmaj guest is truly talented . xxmaj he has a gift for humor . i almost died laughing . xxmaj actually , when i saw this in theaters , i considered walking out because the movie was so dumb . xxmaj but it is dumb</td>\n",
       "      <td>it 's not half bad . \\n\\n xxmaj still waiting for ' surf 3 ' . xxbos xxmaj this has got to be the funniest movie i have seen in forever . xxmaj xxunk xxmaj guest is truly talented . xxmaj he has a gift for humor . i almost died laughing . xxmaj actually , when i saw this in theaters , i considered walking out because the movie was so dumb . xxmaj but it is dumb in</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls_lm = block.dataloaders(path, path=path, bs=128, seq_len=80)\n",
    "dls_lm.show_batch(max_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(\n",
    "    dls_lm, AWD_LSTM, drop_mult=0.3, \n",
    "    metrics=[accuracy, Perplexity()]).to_fp16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Before fine-tuning let's see how it performs at generating a movie review so that we can compare after fine tuning to the imdb corpus__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i liked this movie because of its European subject , which was also a popular film . In France , Henri Franois Basie 's Les Breton Le Mans ( French : La\n",
      "i liked this movie because it was a remake of the Mazes of Swords . While several other characters of Jaws have been adapted for English audiences , the novel was not adapted into the film . The film\n"
     ]
    }
   ],
   "source": [
    "def predict_next(txt, learn):\n",
    "    N_WORDS = 40\n",
    "    N_SENTENCES = 2\n",
    "    preds = [learn.predict(txt, N_WORDS, temperature=0.75) \n",
    "             for _ in range(N_SENTENCES)]\n",
    "    print(\"\\n\".join(preds))\n",
    "\n",
    "predict_next(\"I liked this movie because\", learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What can you observe?__\n",
    "\n",
    "Wrong:\n",
    "- capitalization\n",
    "- punctuation\n",
    "- cut-off sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning the Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like `vision_learner`, `language_model_learner` automatically calls `freeze` when using a pretrained model.\n",
    "\n",
    "So this will only train the new weights, the new embeddings that are in our IMDb vocab, buit not in the pretrained model vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.998230</td>\n",
       "      <td>3.898461</td>\n",
       "      <td>0.300623</td>\n",
       "      <td>49.326469</td>\n",
       "      <td>13:35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 2e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/root/.fastai/data/imdb/models/1epoch.pth')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.save('1epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.765704</td>\n",
       "      <td>3.759862</td>\n",
       "      <td>0.316251</td>\n",
       "      <td>42.942509</td>\n",
       "      <td>14:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.715375</td>\n",
       "      <td>3.695756</td>\n",
       "      <td>0.323597</td>\n",
       "      <td>40.275997</td>\n",
       "      <td>14:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.629537</td>\n",
       "      <td>3.651433</td>\n",
       "      <td>0.328655</td>\n",
       "      <td>38.529819</td>\n",
       "      <td>14:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.556274</td>\n",
       "      <td>3.618382</td>\n",
       "      <td>0.332256</td>\n",
       "      <td>37.277214</td>\n",
       "      <td>14:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.490726</td>\n",
       "      <td>3.595954</td>\n",
       "      <td>0.335296</td>\n",
       "      <td>36.450455</td>\n",
       "      <td>14:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.413398</td>\n",
       "      <td>3.579280</td>\n",
       "      <td>0.337563</td>\n",
       "      <td>35.847736</td>\n",
       "      <td>14:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.343958</td>\n",
       "      <td>3.574382</td>\n",
       "      <td>0.339059</td>\n",
       "      <td>35.672581</td>\n",
       "      <td>14:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.305039</td>\n",
       "      <td>3.567643</td>\n",
       "      <td>0.340169</td>\n",
       "      <td>35.432976</td>\n",
       "      <td>14:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.244978</td>\n",
       "      <td>3.570725</td>\n",
       "      <td>0.340391</td>\n",
       "      <td>35.542358</td>\n",
       "      <td>14:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.224113</td>\n",
       "      <td>3.575514</td>\n",
       "      <td>0.340167</td>\n",
       "      <td>35.712982</td>\n",
       "      <td>14:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = learn.load('1epoch')\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(10, 2e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder('finetuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fined Tuned Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well this imdb-fine_tuned model performs at predicting the next word of a movie review now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i liked this movie because it was so gripping . i was a teenager at the time . The story was very good , and the acting was top notch . It was very realistic and it was told in a sophisticated way\n",
      "i liked this movie because it is a very good movie . This is not quite the kind of movie that the director has made with a very mature and very well written script . This movie has not only a very emotional\n"
     ]
    }
   ],
   "source": [
    "predict_next(\"I liked this movie because\", learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__what can you observe?__\n",
    "\n",
    "\n",
    "good:\n",
    "- Seems like the content seems better\n",
    "  \n",
    "wrong:\n",
    "- punctation\n",
    "- capitalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Classifier DataLoaders - Dataloader defines the task!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `TextBlock.from_folder` no longer has the `is_lm=True` parameter.\n",
    "- We pass the vocab we created for the language model fine-tuning.\n",
    "\n",
    "By passing is_lm=False (or not passing is_lm at all, since it defaults to False) we tell TextBlock that we have regular labeled data, rather than using the next tokens as labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_clas = DataBlock(\n",
    "    blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),CategoryBlock),\n",
    "    get_y = parent_label,\n",
    "    get_items=partial(get_text_files, folders=['train', 'test']),\n",
    "    splitter=GrandparentSplitter(valid_name='test')\n",
    ").dataloaders(path, path=path, bs=128, seq_len=72)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj match 1 : xxmaj tag xxmaj team xxmaj table xxmaj match xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley vs xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley started things off with a xxmaj tag xxmaj team xxmaj table xxmaj match against xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit . xxmaj according to the rules of the match , both opponents have to go through tables in order to get the win . xxmaj benoit and xxmaj guerrero heated up early on by taking turns hammering first xxmaj spike and then xxmaj bubba xxmaj ray . a xxmaj german xxunk by xxmaj benoit to xxmaj bubba took the wind out of the xxmaj dudley brother . xxmaj spike tried to help his brother , but the referee restrained him while xxmaj benoit and xxmaj guerrero</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos * * attention xxmaj spoilers * * \\n\\n xxmaj first of all , let me say that xxmaj rob xxmaj roy is one of the best films of the 90 's . xxmaj it was an amazing achievement for all those involved , especially the acting of xxmaj liam xxmaj neeson , xxmaj jessica xxmaj lange , xxmaj john xxmaj hurt , xxmaj brian xxmaj cox , and xxmaj tim xxmaj roth . xxmaj michael xxmaj canton xxmaj jones painted a wonderful portrait of the honor and dishonor that men can represent in themselves . xxmaj but alas  \\n\\n it constantly , and unfairly gets compared to \" braveheart \" . xxmaj these are two entirely different films , probably only similar in the fact that they are both about xxmaj scots in historical xxmaj scotland . xxmaj yet , this comparison frequently bothers me because it seems</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxbos xxmaj titanic directed by xxmaj james xxmaj cameron presents a fictional love story on the historical setting of the xxmaj titanic . xxmaj the plot is simple , xxunk , or not for those who love plots that twist and turn and keep you in suspense . xxmaj the end of the movie can be figured out within minutes of the start of the film , but the love story is an interesting one , however . xxmaj kate xxmaj winslett is wonderful as xxmaj rose , an aristocratic young lady betrothed by xxmaj cal ( billy xxmaj zane ) . xxmaj early on the voyage xxmaj rose meets xxmaj jack ( leonardo dicaprio ) , a lower class artist on his way to xxmaj america after winning his ticket aboard xxmaj titanic in a poker game . xxmaj if he wants something , he goes and gets it</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls_clas.show_batch(max_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#10) [231,260,148,335,137,363,155,299,145,60]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums_samp = toks200[:10].map(num)\n",
    "nums_samp.map(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using our fine-tuned model (encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, \n",
    "                                metrics=accuracy).to_fp16()\n",
    "\n",
    "learn = learn.load_encoder('finetuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradually Unfreezing the Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to train with discriminative learning rates and gradual unfreezing. In computer vision we often unfreeze the model all at once, but for NLP classifiers, we find that unfreezing a few layers at a time makes a real difference:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.241581</td>\n",
       "      <td>0.180199</td>\n",
       "      <td>0.930840</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 2e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass -2 to freeze_to to freeze all except the last two parameter groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.178601</td>\n",
       "      <td>0.149430</td>\n",
       "      <td>0.945400</td>\n",
       "      <td>00:28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.freeze_to(-2)\n",
    "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can unfreeze a bit more, and continue training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.160318</td>\n",
       "      <td>0.148327</td>\n",
       "      <td>0.947840</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, the whole model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.127381</td>\n",
       "      <td>0.151703</td>\n",
       "      <td>0.945080</td>\n",
       "      <td>00:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.110951</td>\n",
       "      <td>0.153922</td>\n",
       "      <td>0.946520</td>\n",
       "      <td>00:43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reached 94.3% accuracy, which was state-of-the-art performance just three years ago. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export('/workspace/imdb_classifier.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
